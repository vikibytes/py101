{"cells":[{"metadata":{"_uuid":"ed6f044dd5c9bd2e222bb5aa80a0b0d795cca5a8"},"cell_type":"markdown","source":"# Breast cancer prediction with Logistic Regression\n\n### Author\nPiotr Tynecki  \nLast edition: May 4, 2018\n\n### About the Breast Cancer Wisconsin Diagnostic Dataset\nBreast Cancer Wisconsin Diagnostic Dataset (WDBC) consists of features which were computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. Those features describe the characteristics of the cell nuclei found in the image.\n\n![Diagnosing Breast Cancer from Image](https://kaggle2.blob.core.windows.net/datasets-images/180/384/3da2510581f9d3b902307ff8d06fe327/dataset-cover.jpg)\n\nThis dataset has 569 instances: 212 - Malignant and 357 - Benign. It consists of 31 attributes including the class attribute. The attributes description is ten real-valued features which are computed for each cell nucleus. These features include: Texture, Radius, Perimeter, Smoothness, Area, Concavity, Compactness, Symmetry, Concave points and Fractal dimension.\n\nIn this document I demonstrate an automated methodology to predict if a sample is benign or malignant."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"5e88fa1c687028c3a7044823966eaea103a0d39a"},"cell_type":"code","source":"import operator\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, roc_auc_score, classification_report\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"424b81b0e14ee82fc1760688e42e4d77fb714452"},"cell_type":"markdown","source":"### Step 1: Exploratory Data Analysis (EDA)\nEDA is for seeing what the data can tell us beyond the formal modeling or hypothesis testing task. It let us to summarize data main characteristics."},{"metadata":{"trusted":false,"_uuid":"06c060ce6486be786ce3ed4abc4f4455099d2ca9"},"cell_type":"code","source":"breast_cancer = pd.read_csv('../input/data.csv')\nbreast_cancer.head()","execution_count":2,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"cc954f81f0069a807dd6caab9fc06d30daad50c4"},"cell_type":"code","source":"breast_cancer.info()","execution_count":3,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"4363857962df42ca10cfae8c950c9840e27db7e2"},"cell_type":"code","source":"breast_cancer.shape","execution_count":4,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"686a80979517c7a73330546e3b95c1353de8bb7e"},"cell_type":"code","source":"breast_cancer.describe()","execution_count":5,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"5a058489eec5eb62ed8577f7a9e4f53a4c34a3e2"},"cell_type":"code","source":"breast_cancer.groupby('diagnosis').size()","execution_count":6,"outputs":[]},{"metadata":{"_uuid":"682e244553c2a41bd24f1674b3bf24b64d68cc3e"},"cell_type":"markdown","source":"#### Data quality checks"},{"metadata":{"trusted":false,"_uuid":"6724a6c08af005c5d58b802999e13ea2df443240"},"cell_type":"code","source":"breast_cancer.isnull().sum()","execution_count":7,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"4bca9d5a48f3dff3df6e5dd350dc4f84cf5f5acc"},"cell_type":"code","source":"for field in breast_cancer.columns:\n    amount = np.count_nonzero(breast_cancer[field] == 0)\n    \n    if amount > 0:\n        print('Number of 0-entries for \"{field_name}\" feature: {amount}'.format(\n            field_name=field,\n            amount=amount\n        ))","execution_count":8,"outputs":[]},{"metadata":{"_uuid":"fa3fb9557fe32571752b1500f7cd3b704d6f2085"},"cell_type":"markdown","source":"### Step 2: Feature Engineering"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"08dab7035d9e560c4577c371ae2c7e56560c2bf5"},"cell_type":"code","source":"# Features \"id\" and \"Unnamed: 32\" are not useful \nfeature_names = breast_cancer.columns[2:-1]\nX = breast_cancer[feature_names]\n# \"diagnosis\" feature is our class which I wanna predict\ny = breast_cancer.diagnosis","execution_count":9,"outputs":[]},{"metadata":{"_uuid":"f5f5b33c28972f9c1540d67b4b9f3aa96563e916"},"cell_type":"markdown","source":"#### Transforming the prediction target"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"02bf027e43d42df7d44c655c03fd397055f7643e"},"cell_type":"code","source":"class_le = LabelEncoder()\n# M -> 1 and B -> 0\ny = class_le.fit_transform(breast_cancer.diagnosis.values)","execution_count":10,"outputs":[]},{"metadata":{"_uuid":"06a493a90c5ee400ef85ca6bbff9c274e7b8eac2"},"cell_type":"markdown","source":"#### Correlation Matrix\nA matrix of correlations provides useful insight into relationships between pairs of variables."},{"metadata":{"trusted":false,"_uuid":"8511ee3e2466f331dee22b26c55ffe5a1658426c"},"cell_type":"code","source":"sns.heatmap(\n    data=X.corr(),\n    annot=True,\n    fmt='.2f',\n    cmap='RdYlGn'\n)\n\nfig = plt.gcf()\nfig.set_size_inches(20, 16)\n\nplt.show()","execution_count":11,"outputs":[]},{"metadata":{"_uuid":"064fb1f5f3c67f246455cbec5f9c07d2841ae268"},"cell_type":"markdown","source":"### Step 3: Automated Logistic Regression performance evaluation with Pipeline and GridSearchCV \n\nFor this case study I decided to use [LogisticRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) classifier.\n\n#### Model Parameter Tuning\n[GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) returns the set of parameters which have an imperceptible impact on model evaluation. Model parameter tuning with other steps like data standardization, principal component analysis (PCA) and cross-validation splitting strategy can be automated by [Pipeline](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) class.\n\n#### Data standardization\n[Standardize features](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) by removing the mean and scaling to unit variance.\n\n####  Principal component analysis (PCA)\nThe main goal of a PCA analysis is to identify patterns in data. PCA aims to detect the correlation between variables. If a strong correlation between variables exists, the attempt to reduce the dimensionality only makes sense."},{"metadata":{"_uuid":"49c8d187d5ef50d9e924d0aa645599262125b9d5"},"cell_type":"markdown","source":"Let's start with defining the Pipeline instance. In this case I used `StandardScaler` for preprocesing, `PCA` for feature selection and `LogisticRegression` for classification."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"9741951fc05dbdbd78a92c6822adfed160e05606"},"cell_type":"code","source":"pipe = Pipeline(steps=[\n    ('preprocess', StandardScaler()),\n    ('feature_selection', PCA()),\n    ('classification', LogisticRegression())\n])","execution_count":12,"outputs":[]},{"metadata":{"_uuid":"afb057ca2b8501a5c0c59f2e942baa7ab3228463"},"cell_type":"markdown","source":"Next, I needed to prepare attributes with values for above steps which wanna to check by the model parameter tuning process: 5 values for `C` (regularization strength for classifier) and the generator of numbers for `n_components` (number of components to keep for feature selector)."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"71903ee1191c335876bc7dbd4473dc7465851bc3"},"cell_type":"code","source":"c_values = [0.1, 1, 10, 100, 1000]\nn_values = range(2, 31)\nrandom_state = 42","execution_count":13,"outputs":[]},{"metadata":{"_uuid":"a73cebd33a0b68c1e11bcde397ceb89947177e58"},"cell_type":"markdown","source":"Next, I needed to prepare supported combinations for classifier parameters including above attributes. In LogisticRegression case I stayed with two scenarios."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"b0578dda28c28bf3626793cfcbff2bf8b923578a"},"cell_type":"code","source":"log_reg_param_grid = [\n    {\n        'feature_selection__random_state': [random_state],\n        'feature_selection__n_components': n_values,\n        'classification__C': c_values,\n        'classification__penalty': ['l1'],\n        'classification__solver': ['liblinear'],\n        'classification__multi_class': ['ovr'],\n        'classification__random_state': [random_state]\n    },\n    {\n        'feature_selection__random_state': [random_state],\n        'feature_selection__n_components': n_values,\n        'classification__C': c_values,\n        'classification__penalty': ['l2'],\n        'classification__solver': ['liblinear', 'newton-cg', 'lbfgs'],\n        'classification__multi_class': ['ovr'],\n        'classification__random_state': [random_state]\n    }\n]","execution_count":14,"outputs":[]},{"metadata":{"_uuid":"68f805863c9a73bb73541cd3e2afe2f871ac34fd"},"cell_type":"markdown","source":"Next, I needed to prepare cross-validation splitting strategy object with `StratifiedKFold` and passed it with others to `GridSearchCV`. In that case for evaluation I used `accuracy` metric."},{"metadata":{"trusted":false,"_uuid":"6e549e7707d01b4be8505ebb777e53bf36fed484"},"cell_type":"code","source":"strat_k_fold = StratifiedKFold(\n    n_splits=10,\n    random_state=42\n)\n\nlog_reg_grid = GridSearchCV(\n    pipe,\n    param_grid=log_reg_param_grid,\n    cv=strat_k_fold,\n    scoring='accuracy'\n)\n\nlog_reg_grid.fit(X, y)\n\n# Best LogisticRegression parameters\nprint(log_reg_grid.best_params_)\n# Best score for LogisticRegression with best parameters\nprint('Best score for LogisticRegression: {:.2f}%'.format(log_reg_grid.best_score_ * 100))\n\nbest_params = log_reg_grid.best_params_","execution_count":15,"outputs":[]},{"metadata":{"_uuid":"df229a05987d246af5e99d504b1c9c2d0ca9a73d"},"cell_type":"markdown","source":"#### Model evaluation\n\nFinally, all of the best parameters values were passed to new feature selection and classifier instances."},{"metadata":{"trusted":false,"_uuid":"aa480386b9c0f892dbe343b55bc7ace142ea7854"},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(\n    X,\n    y,\n    random_state=42,\n    test_size=0.20\n)\n\nstd_scaler = StandardScaler()\n\nX_train_std = std_scaler.fit_transform(X_train)\nX_test_std = std_scaler.transform(X_test)\n\npca = PCA(\n    n_components=best_params.get('feature_selection__n_components'),\n    random_state=best_params.get('feature_selection__random_state')\n)\n\nX_train_pca = pca.fit_transform(X_train_std)\nX_test_pca = pca.transform(X_test_std)\n\nprint(pca.explained_variance_ratio_)\nprint('\\nPCA sum: {:.2f}%'.format(sum(pca.explained_variance_ratio_) * 100))\n\nlog_reg = LogisticRegression(\n    C=best_params.get('classification__C'),\n    penalty=best_params.get('classification__penalty'),\n    solver=best_params.get('classification__solver'),\n    multi_class=best_params.get('classification__multi_class'),\n    random_state=best_params.get('classification__random_state'),\n)\n\nlog_reg.fit(X_train_pca, y_train)\n\nlog_reg_predict = log_reg.predict(X_test_pca)\nlog_reg_predict_proba = log_reg.predict_proba(X_test_pca)[:, 1]\n\nprint('LogisticRegression Accuracy: {:.2f}%'.format(accuracy_score(y_test, log_reg_predict) * 100))\nprint('LogisticRegression AUC: {:.2f}%'.format(roc_auc_score(y_test, log_reg_predict_proba) * 100))\nprint('LogisticRegression Classification report:\\n\\n', classification_report(y_test, log_reg_predict))\nprint('LogisticRegression Training set score: {:.2f}%'.format(log_reg.score(X_train_pca, y_train) * 100))\nprint('LogisticRegression Testing set score: {:.2f}%'.format(log_reg.score(X_test_pca, y_test) * 100))","execution_count":16,"outputs":[]},{"metadata":{"_uuid":"70fff321c0df253da0891147fe8bb88577b2423f"},"cell_type":"markdown","source":"#### Confusion Matrix\n\nAlso known as an Error Matrix, is a specific table layout that allows visualization of the performance of an algorithm. The table have two rows and two columns that reports the number of False Positives (FP), False Negatives (FN), True Positives (TP) and True Negatives (TN). This allows more detailed analysis than accuracy."},{"metadata":{"trusted":false,"_uuid":"2d9af85daed62ce992f222f6ec03e67af7d54572"},"cell_type":"code","source":"outcome_labels = sorted(breast_cancer.diagnosis.unique())\n\n# Confusion Matrix for LogisticRegression\nsns.heatmap(\n    confusion_matrix(y_test, log_reg_predict),\n    annot=True,\n    xticklabels=outcome_labels,\n    yticklabels=outcome_labels\n)","execution_count":17,"outputs":[]},{"metadata":{"_uuid":"a66cec94ecefe3002555db2e179ef21d04be7053"},"cell_type":"markdown","source":"#### Receiver Operating Characteristic (ROC)\n\n[ROC curve](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html) is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied."},{"metadata":{"trusted":false,"_uuid":"0b49929fef464290791d6134803a0195eae1d473"},"cell_type":"code","source":"# ROC for LogisticRegression\nfpr, tpr, thresholds = roc_curve(y_test, log_reg_predict_proba)\n\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr, tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.rcParams['font.size'] = 12\nplt.title('ROC curve for LogisticRegression')\nplt.xlabel('False Positive Rate (1 - Specificity)')\nplt.ylabel('True Positive Rate (Sensitivity)')\nplt.grid(True)","execution_count":18,"outputs":[]},{"metadata":{"_uuid":"e959de294ac84b9a531e7fd0d0a41c53a6013749"},"cell_type":"markdown","source":"#### F1-score after 10-fold cross-validation"},{"metadata":{"trusted":false,"_uuid":"7a824d55aa8da32adb85154239bc60c06e90a950"},"cell_type":"code","source":"strat_k_fold = StratifiedKFold(\n    n_splits=10,\n    random_state=42\n)\n\nstd_scaler = StandardScaler()\n\nX_std = std_scaler.fit_transform(X)\nX_pca = pca.fit_transform(X_std)\n\nfe_score = cross_val_score(\n    log_reg,\n    X_pca,\n    y,\n    cv=strat_k_fold,\n    scoring='f1'\n)\n\nprint(\"LogisticRegression: F1 after 10-fold cross-validation: {:.2f}% (+/- {:.2f}%)\".format(\n    fe_score.mean() * 100,\n    fe_score.std() * 2\n))","execution_count":19,"outputs":[]},{"metadata":{"_uuid":"1cc37cc8c5c66519a02af5a20e9a25e1aecd5079"},"cell_type":"markdown","source":"### Final step: Conclusions\n\nAfter the application of data standardization, staying with 16 numbers of componens for feature selector and tuning the classifier parameters I achieved the following results:\n\n* Accuracy: 99%\n* F1-score: 99%\n* Precision: ~100%\n* Recall: ~99%\n\nI would love to knows your comments and other tuning proposals for that study case."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":1}